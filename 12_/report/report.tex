\documentclass[dvipdfmx]{jsarticle}

\usepackage{ascmac}
\usepackage{url}
\usepackage[dvipdfmx]{hyperref}
\usepackage{pxjahyper}
\usepackage[dvipdfmx]{graphicx}
\usepackage{float}
\usepackage{listings,jlisting}

\hypersetup{
  colorlinks=true,
  urlcolor=cyan,
  linkcolor=black
}

\lstset{
  basicstyle={\ttfamily},
  identifierstyle={\small},
  commentstyle={\smallitshape},
  keywordstyle={\small\bfseries},
  ndkeywordstyle={\small},
  stringstyle={\small\ttfamily},
  frame={tb},
  breaklines=true,
  columns=[l]{fullflexible},
  numbers=left,
  xrightmargin=0zw,
  xleftmargin=3zw,
  numberstyle={\scriptsize},
  stepnumber=1,
  numbersep=1zw,
  lineskip=-0.5ex
}


\begin{document}

\section{実験目的・課題}

\begin{itemize}
  \item 単純な二次元データの分類
  \item 動物園の動物データ(多次元データ)の分類
  \item 食べ物の画像データの分類
\end{itemize}
を行う。

\section{実装方法}

\subsection{クラスタリングの手法について}

データのクラスタリングをする手法には以下のようなものがある。
\begin{itemize}
  \item 最短距離法
  \item 最長距離法
  \item 重心法
  \item 群平均法
  \item K-means法
\end{itemize}

最短距離法とは2つのクラスター同士について、
最も距離が小さいものから順にグループにしていくものである。
計算量が少ないが、ある1つのクラスタにひとつづつ吸収されてしまう
鎖効果が起こってしまうことがある。

最長距離法はクラスター間の距離をそのクラスター間の中で
最も遠いデータとし、その距離の小さいものから
クラスターにしていく手法である。
分類感度は高いがクラスター同士が離れてしまう拡散現象が起きることがある。

重心法はクラスターに属するデータの重心を決定し、
重心間の距離の小さい順にクラスターを
形成していく手法である。
計算量は多いが分類感度が良い。

群平均法は2つのクラスターに含まれるすべてのデータ間の距離
を計算し、その平均の値をクラスター間の距離として
クラスタリングをする手法である。
鎖効果や拡散現象をおこしにくい。

K-means法とはデータ数をn、クラスタ数をkとして、
以下のような手順で表されるアルゴリズムである。
\begin{enumerate}
  \item 各データ$d_i(i=1,\dots,n)$に対してランダムにクラスタを割り振る
  \item 各クラスタの中心$m_j(j=1,\dots,k)$を求める
  \item 各$d_i$と各$m_i$との距離を求め$d_i$を最も近いクラスタに割り当てなおす
  \item クラスタの割り当てが変化しなかった場合、終了する。\\そうでない場合手順2に戻る
\end{enumerate}
結果は、最初に割り振られたクラスタに依存し、
1回の結果で最良のものが得られるとは限らない。

\subsection{二次元データの分類}

二次元データ間の距離をユークリッド距離
$d = \sqrt{x^2 + y^2}$
で定義する。
これを最短距離法で分類することを考える。
最短距離法では単に短い距離のものから順にクラスタにまとめていけばよい。
これはクラスカル法で最小全域木を構築するのと
同様の手順で行うことができる。

クラスカル法とはグラフ理論において重み付き連結グラフの
最小全域木を求めることが出来る以下のようなアルゴリズムのことである。
\begin{enumerate}
  \item $cost(e_0) \leq cost(e_1) \leq \dots \leq cost(e_m)$が成り立つように辺を並び替える
  \item $T:=(V(G),\emptyset)$とする
  \item i=1..mについて、$E(T) \cup \{e_i\}$が閉路を含まないならば$E(T) := E(T) \cup \{e_i\}$とする
\end{enumerate}
二次元データを頂点、各点間の距離を重みとした辺からなるグラフを考える。
最初はN個の木(森)が存在し、このアルゴリズムが終了したときは1つの木だけが残っている。
この手順の途中で木の数がkになったときにアルゴリズムを終了すると、
k個のクラスタに分類することが出来ている。


\subsection{多次元データの分類}

二次元データの分類の時と同様にクラスカル法を実行することで
最小距離法での分類を行う。
このときn次元データaとbの間の距離は
$\displaystyle d_{ab} = \sqrt{ \sum_{i=1}^{n}w_i(a_i - b_i)^2 }$
として計算した。ここで$w_i$は各系列に対する重みである。

\subsection{画像データの分類}

画像データを次元の小さい多次元データに圧縮することで、
前節と同様に分類をすることができるようになる。

画像のデータはPythonのOpenCVではRGBを表す3要素からなる
配列の二次元配列である。
このRGBの値は$256^3=16777216$通り存在する。
各色についてその色が画像にいくつ含まれているかを
カウントした16777216次元データを比べることで画像間の
距離とすることでクラスタリングに必要な情報が得られる。
しかし、16777216個のデータを比較するのはデータの量が多すぎるため
困難である。
ここで、各画素の値を64で割り、0から255の値を0から3までの値に変換する。
こうすることで各画素を$4^3=64$通りの色に圧縮することが出来る。

画像を64次元データに圧縮することができたので最短距離法によって
画像の分類を実行する。

画像の分類結果を確認するために、Pythonのsubprocessパッケージ
のrun関数を使い、分類のたびに新たにディレクトリを作成し、
そこへ元画像のファイルをコピーした。

\section{結果と考察}

\begin{thebibliography}{10}
  \bibitem{1}　クラスター分析の手法②（階層クラスター分析）

  \url{https://www.albert2005.co.jp/knowledge/data_mining/cluster/hierarchical_clustering}

  \bibitem{2} k-means法を理解する

  \url{https://qiita.com/g-k/items/0d5d22a12a4507ecbf11}
\end{thebibliography}
\end{document}